\documentclass[12pt, a4paper, portrait]{article}
\usepackage[margin = 2cm]{geometry}
% Vertical text spacing
\parindent = 0cm \parskip = 0cm
% Table spacing
\newcommand\TS{\rule{0pt}{2.6ex}}         % Top strut
\newcommand\BS{\rule[-0.9ex]{0pt}{0pt}}   % Bottom strut
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\TS\BS\arraybackslash}m{#1}}   % Align left
\newcolumntype{C}[1]{>{\centering  \TS\BS\arraybackslash}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft \TS\BS\arraybackslash}m{#1}}   % Align right
% Equations
\usepackage{amsmath, bm, bbold}

\begin{document}

\section*{Implementation}
We select the best attribute at each node by computing its information gain, which is the decrease in entropy of the dataset after it has been split on that attribute.
\[ \text{Gain(Attribute)} = I(p, n) - \left[ \frac{p_0 + n_0}{p + n} \, I(p_0, n_0) + \frac{p_1 + n_1}{p + n} \, I(p_1, n_1) \right] \]
\begin{align*}
    p &= \text{Number of positive examples before split} \\
    n &= \text{Number of negative examples before split} \\
    p_k &= \text{Number of positive examples with attribute} = k \\
    n_k &= \text{Number of negative examples with attribute} = k \\
    I(p, n) &= - \frac{p}{p+n} \log \frac{p}{p+n} - \frac{n}{p+n} \log \frac{n}{p+n}
\end{align*}
We could have used other metrics such as the Gini impurity:
\[ I(p, n) = \frac{p}{p+n} \left( 1 - \frac{p}{p+n} \right) + \frac{n}{p+n} \left( 1 - \frac{n}{p+n} \right) \]
which should give similar results to entropy since $-x \log x \approx x (1-x)$ when $x \approx 0$.\par
\bigskip
Cross validation:\par
1. Shuffle the dataset and split it into $K = 10$ parts\par
2. For each $k \in \{1, \dots, K\}$ we train the decision tree on the dataset excluding part $k$ and then test the tree on part $k$.

\section*{Evaluation}
Confusion matrix:
\begin{center}
\begin{tabular} { C{1.7cm} | C{1.5cm} C{1.5cm} C{1.3cm} C{1.7cm} C{1.5cm} C{1.5cm} }
    & Anger & Disgust & Fear & Happiness & Sadness & Surprise \\ \hline
    Anger     &   &   &   &   &   &   \\
    Disgust   &   &   &   &   &   &   \\
    Fear      &   &   &   &   &   &   \\
    Happiness &   &   &   &   &   &   \\
    Sadness	  &   &   &   &   &   &   \\
    Surprise  &   &   &   &   &   &
\end{tabular}
\end{center}
Precision\par
Recall\par
$F_1$ score\par

\section*{Miscellaneous}

\textit{Noisy-Clean Datasets Question}\par
\bigskip
The noisy dataset has lower performance.\par
\bigskip

\textit{Ambiguity Question}\par
\bigskip

\textit{Pruning Question}\par
\bigskip

\end{document}
